# Отчет по домашнему заданию: Сверточные сети и кастомные слои

Целью данной работы является практическое исследование сверточных нейронных сетей (CNN). В ходе экспериментов мы сравним эффективность различных архитектур на задаче классификации изображений, проанализируем влияние их компонентов и изучим процесс создания кастомных слоев

## Задание 1: Сравнение архитектур

### 1.1 Сравнение моделей на MNIST

На первом этапе мы сравнили производительность трех архитектур на классической задаче распознавания рукописных цифр из набора данных MNIST. Цель — наглядно продемонстрировать преимущества сверточных сетей над полносвязными.

Были обучены и протестированы следующие модели:

- **Полносвязная сеть (FC_MNIST)**: Стандартная нейронная сеть из нескольких полносвязных слоев
- **Простая сверточная сеть (SimpleCNN)**: Компактная CNN с двумя сверточными и двумя полносвязными слоями
- **Сеть с остаточными блоками (ModelWithBasicBlocks)**: Глубокая сверточная сеть, построенная на основе базовых остаточных блоков

#### Результаты по MNIST

Все модели обучались в течение 5 эпох. Сводные результаты производительности представлены в таблице ниже.

| Модель (MNIST) | Точность (Тест) | Параметры | Время обучения (сек) |
|----------------|-----------------|-----------|---------------------|
| FC_MNIST | 97.19% | 567,434 | 119.69 |
| SimpleCNN | 99.20% | 421,642 | 126.21 |
| ModelWithBasicBlocks | 98.58% | 11,172,810 | 232.16 |

На графиках ниже можно визуально сравнить, как менялись точность и потери у каждой модели в процессе обучения.
| Точность | Потери |
|:---------:|:---------:|
| ![Сравнение моделей на MNIST по точности](homework/plots/mnist_comparison/сравнение_моделей_MNIST_по_точности.png) | ![Сравнение моделей на MNIST по потерям](homework/plots/mnist_comparison/сравнение_моделей_MNIST_по_потерям.png) |



#### Кривые обучения отдельных моделей

![FC_MNIST - кривые обучения](homework/plots/mnist_comparison/FC_MNIST_learning_curves.png)

![SimpleCNN - кривые обучения](homework/plots/mnist_comparison/SimpleCNN_learning_curves.png)

![ModelWithBasicBlocks - кривые обучения](homework/plots/mnist_comparison/ModelWithBasicBlocks_learning_curves.png)

#### Выводы по MNIST

**Эффективность CNN**: Простая сверточная сеть SimpleCNN показала наилучший результат (99.20%), значительно обогнав полносвязную сеть. Она достигла этого, имея на 25% меньше параметров (421 тыс. против 567 тыс. у FC_MNIST). Это показывает преимущество CNN: способность эффективно извлекать пространственные признаки с помощью общих весов в сверточных ядрах.

**Избыточность сложных моделей**: Глубокая сеть ModelWithBasicBlocks с остаточными связями показала себя хуже, чем простая CNN. Несмотря на свою сложность и огромное количество параметров (11.1 млн), она не смогла достичь лучшей точности -> для относительно простых датасетов, как MNIST, очень глубокие архитектуры могут быть избыточны. Их сложнее оптимизировать, и они не всегда дают лучший результат.

### 1.2 Сравнение моделей на CIFAR-10

Чтобы проверить модели на более сложной задаче, провели сравнение на датасете CIFAR-10, который состоит из цветных изображений 10 различных классов. Здесь мы также добавили модель с регуляризацией (Dropout) для борьбы с переобучением.

Сравнивались модели:

- **FC_CIFAR**: Глубокая полносвязная сеть
- **ModelWithBasicBlocks**: Сеть с остаточными блоками (аналог ResNet-18)
- **ModelWithRegularization**: Та же сеть, но с добавлением слоя Dropout перед классификатором

#### Результаты по CIFAR-10

Модели обучались 10 эпох.

| Модель (CIFAR-10) | Точность (Тест) | Разрыв (Train-Test) | Параметры | Время обучения (сек) |
|-------------------|-----------------|-------------------|-----------|---------------------|
| FC_CIFAR | 36.22% | -4.59% | 9,050,378 | 207.41 |
| ModelWithBasicBlocks | 84.00% | 12.97% | 11,173,962 | 458.24 |
| ModelWithRegularization | 83.49% | 12.73% | 11,173,962 | 460.40 |

Графики сравнения показывают колоссальный разрыв в производительности между полносвязной и сверточными архитектурами.

| Точность | Потери |
|:---------:|:---------:|
| ![Сравнение моделей на CIFAR-10 по точности](homework/plots/cifar_comparison/сравнение_моделей_CIFAR-10_по_точности.png) | ![Сравнение моделей на CIFAR-10 по потерям](homework/plots/cifar_comparison/сравнение_моделей_CIFAR-10_по_потерям.png) |



#### Анализ переобучения и ошибок

Кривые обучения для сверточных сетей показывают явное переобучение: кривая потерь на обучающей выборке (Train Loss) продолжает падать, в то время как на тестовой (Test Loss) она не меняется или даже начинает расти.

![FC_CIFAR - кривые обучения](homework/plots/cifar_comparison/FC_CIFAR_learning_curves.png)

![ModelWithBasicBlocks - кривые обучения](homework/plots/cifar_comparison/ModelWithBasicBlocks_learning_curves.png)

![ModelWithRegularization - кривые обучения](homework/plots/cifar_comparison/ModelWithRegularization_learning_curves.png)

Матрица ошибок для лучшей модели (ModelWithBasicBlocks) показывает, какие классы модель путает чаще всего. Например, заметна путаница между классами кошка и собака, а также птица и самолет. В то же время классы машина и корабль распознаются очень уверенно

![Матрица ошибок для CIFAR-10](homework/plots/cifar_comparison/ModelWithBasicBlocks_confusion_matrix.png)

#### Выводы по CIFAR-10

**Критическая важность CNN**: На сложных цветных изображениях полносвязная сеть практически бесполезна (точность ~36%). Только сверточные сети способны вытаскивать необходимые комбинации признаков для достижения приемлемого результата (>80%).

**Переобучение — главная проблема**: Глубокие CNN сильно склонны к переобучению на датасетах среднего размера, как CIFAR-10. Это видно по большому разрыву между точностью на обучающей и тестовой выборках (~13%).

**Регуляризация помогает, но не всегда**: Модель с Dropout показала более стабильные кривые обучения, однако в данном конкретном запуске итоговая точность оказалась чуть ниже, чем у модели без регуляризации. Это говорит о том, что эффект от регуляризации может требовать более длительного обучения или подбора гиперпараметров.

## Задание 2: Анализ архитектур CNN

### 2.1 Влияние размера ядра свертки

В этом эксперименте мы исследовали, как размер сверточного ядра (3x3, 5x5, 7x7) влияет на производительность, качество извлекаемых признаков и количество параметров. Эксперимент проводился на MNIST.

#### Результаты по размеру ядра

Ключевой особенностью было поддержание примерно одинакового числа параметров в моделях за счет изменения количества каналов.

| Модель | Точность (Тест) | Параметры |
|--------|-----------------|-----------|
| CNN_Kernel_3x3 | 98.45% | 50,186 |
| CNN_Kernel_5x5 | 98.12% | 40,170 |
| CNN_Kernel_7x7 | 97.89% | 30,154 |

![Сравнение моделей по размеру ядра](homework/plots/architecture_analysis/kernel_size/сравнение_моделей_по_размеру_ядра.png)

#### Визуализация активаций первого слоя

Карты активаций показывают, какие признаки "видит" сеть после первого сверточного слоя.

| Ядро 3x3 | Ядро 5x5 | Ядро 7x7 |
|:---------:|:---------:|:---------:|
| ![Активации CNN с ядром 3x3](homework/plots/architecture_analysis/kernel_size/активации_CNN_Kernel_3x3_conv1.png) | ![Активации CNN с ядром 5x5](homework/plots/architecture_analysis/kernel_size/активации_CNN_Kernel_5x5_conv1.png) | ![Активации CNN с ядром 7x7](homework/plots/architecture_analysis/kernel_size/активации_CNN_Kernel_7x7_conv1.png) |

**Анализ карт активаций:**

- **Ядра 3x3** выделяют самые мелкие и четкие детали: тонкие грани, углы, концы линий
- **Ядра 5x5** создают более "размытые" карты, захватывая более общие контуры
- **Ядра 7x7** реагируют на еще более крупные области и общие формы, теряя мелкие детали

#### Выводы по размеру ядра

**Оптимальность ядер 3x3**: В данном эксперименте модель с ядрами 3x3 показала наилучшую точность (98.45%)

**Качество признаков**: Визуализация подтверждает, что маленькие ядра лучше подходят для обнаружения локальных признаков (грани, текстуры), в то время как большие — для общих форм.

## Задание 3: Эксперименты с кастомными слоями

### 3.1 Реализация кастомных слоев

В рамках этого задания реализовали и протестировали несколько кастомных слоев:

- **Swish активация**: Функция активации вида `f(x) = x * sigmoid(x)`
- **Механизм внимания (Attention)**: Channel attention механизм для выделения важных каналов
- **GatedConv2d**: Сверточный слой с гейтинг-механизмом
- **L2Pool2d**: Пулинг-слой, реализующий L2-норму

Все слои были успешно протестированы на тестовых тензорах и показали корректную работу.

### 3.2 Сравнение остаточных блоков

Мы сравнили производительность двух типов остаточных блоков:

- **Базовые остаточные блоки (Basic Residual Blocks)**: Классические блоки с двумя сверточными слоями
- **Бутылочные остаточные блоки (Bottleneck Residual Blocks)**: Блоки с тремя сверточными слоями и сужением каналов

#### Результаты сравнения

| Модель | Точность | Параметры | Время обучения |
|--------|----------|-----------|----------------|
| ModelWithBasicBlocks | 84.64% | 11,173,962 | 449.52 |
| ModelWithBottleneckBlocks | 80.50% | 13,958,986 | 731.39 |

![Сравнение типов блоков по точности](homework/plots/custom_experiments/сравнение_типов_блоков_по_точности.png)

#### Выводы по кастомным слоям

**Преимущество базовых блоков**: Базовые остаточные блоки показали лучшую производительность (84.64% vs 80.50%) при меньшем количестве параметров и времени обучения. Это объясняется тем, что для датасета CIFAR-10 сложность бутылочных блоков может быть избыточной.

**Эффективность кастомных слоев**: Все реализованные кастомные слои работают корректно и могут быть интегрированы в существующие архитектуры. Механизм внимания и гейтинг добавляют нелинейность без изменения размерности входных данных.

**Временные характеристики**: Бутылочные блоки требуют значительно больше времени обучения (731.39 сек vs 449.52 сек).

## Общие выводы

### Ключевые результаты

1. **Эффективность CNN**: Сверточные сети значительно превосходят полносвязные на задачах классификации изображений, особенно на сложных датасетах как CIFAR-10.

2. **Важность архитектуры**: Простая и правильно подобранная архитектура (SimpleCNN на MNIST) может превзойти гораздо более глубокую и сложную модель.

3. **Проблема переобучения**: Глубокие CNN склонны к переобучению, что требует применения техник регуляризации.

4. **Оптимальность ядер 3x3**: Небольшие сверточные ядра обеспечивают лучшую производительность и качество извлекаемых признаков.

5. **Практичность кастомных слоев**: Реализованные кастомные слои работают корректно и могут быть полезны для специфических задач.
7. **Нет универсальных решений**: Выбор архитектурных компонентов (размер ядра, тип блока) сильно зависит от контекста задачи. То, что хорошо работает на очень глубоких сетях (например, Bottleneck блоки), может быть неэффективным для сетей средней глубины.
### Временные характеристики

- Полносвязные сети обучаются быстрее на простых данных
- Глубокие CNN требуют значительно больше времени
- Бутылочные блоки увеличивают время обучения в 1.6 раза
- Регуляризация практически не влияет на время обучения

Данная работа наглядно продемонстрировала фундаментальные преимущества сверточных нейронных сетей над полносвязными в задачах компьютерного зрения. Мы подтвердили, что для сложных данных, таких как цветные изображения, CNN являются более эффективным решением.
